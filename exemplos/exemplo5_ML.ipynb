{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "760686b7",
   "metadata": {},
   "source": [
    "# GroupBy and Aggregate Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80558b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import PySpark and create a SparkSession\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"ImportData\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a07e8a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://msi-sobreiro.mshome.net:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>ImportData</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x261ea027770>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a14aa7eb",
   "metadata": {},
   "source": [
    "# Ler o ficheiro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a98bacff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read a CSV file and create a DataFrame\n",
    "df_customers=spark.read.option(\"header\",\"true\").csv('./data/customers_data.csv',inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9419a293",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+---+---------+-------+-------+-------+--------+-----+---------+----+---------+------+-------+\n",
      "|  Id|age|sex|dayswfreq|tbilled|maccess|freeuse|nentries|cfreq|nrenewals|cref|startDate|months|dropout|\n",
      "+----+---+---+---------+-------+-------+-------+--------+-----+---------+----+---------+------+-------+\n",
      "|0...| 23|  1|        7|   37.6|   1.35|      0|       6|    7|        0|   0|     2...|     1|      1|\n",
      "|0...| 34|  1|      328|   2...|   0.54|      0|      39|    7|        2|   0|     2...|    19|      0|\n",
      "|0...| 24|  0|        3|   1...|    0.8|      0|      28|    7|        0|   0|     2...|     8|      1|\n",
      "|0...| 20|  1|       41|   71.6|    1.0|      0|      13|    7|        0|   0|     2...|     3|      1|\n",
      "|0...| 21|  1|       18|   1...|   0.08|      0|       7|    7|        3|   0|     2...|    24|      1|\n",
      "|0...| 20|  0|       38|   1...|   0.33|      0|      11|    7|        2|   0|     2...|    10|      1|\n",
      "|0...| 26|  1|      279|   53.2|   0.16|      0|       6|    7|        1|   0|     2...|     9|      1|\n",
      "|0...| 44|  0|       45|   3...|   0.93|   NULL|      52|    7|        2|   0|     2...|    15|      1|\n",
      "|0...| 20|  0|       56|   2...|   0.31|   NULL|      27|    7|        3|   0|     2...|    22|      1|\n",
      "|0...| 21|  0|        4|   73.6|   2.26|   NULL|      21|    7|        0|   0|     2...|     3|      1|\n",
      "+----+---+---+---------+-------+-------+-------+--------+-----+---------+----+---------+------+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_customers.show(10,truncate=4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "78a0a4b0",
   "metadata": {},
   "source": [
    "# Implementar o algoritmo\n",
    "\n",
    "* instalar as bibliotecas:\n",
    "    * conda install -c conda-forge graphviz\n",
    "    * conda install -c conda-forge python-graphviz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf0d72b5",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'graphviz'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mml\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclassification\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DecisionTreeClassifier\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mml\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m VectorAssembler\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgraphviz\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Digraph\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'graphviz'"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from graphviz import Digraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "81ff7e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the features and label columns\n",
    "feature_cols = [\"age\", \"sex\", \"dayswfreq\",\"tbilled\",\"maccess\",\"freeuse\",\"nentries\",\"cfreq\",\"nrenewals\",\"months\"]\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "data = assembler.transform(df_customers).select(\"features\", \"dropout\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/26 22:50:28 ERROR Executor: Exception in task 0.0 in stage 3.0 (TID 3)\n",
      "org.apache.spark.SparkException: [FAILED_EXECUTE_UDF] Failed to execute user defined function (`VectorAssembler$$Lambda$3228/0x000000084139d840`: (struct<age_double_VectorAssembler_16aac7a84706:double,sex_double_VectorAssembler_16aac7a84706:double,dayswfreq_double_VectorAssembler_16aac7a84706:double,tbilled:double,maccess:double,freeuse_double_VectorAssembler_16aac7a84706:double,nentries_double_VectorAssembler_16aac7a84706:double,cfreq_double_VectorAssembler_16aac7a84706:double,nrenewals_double_VectorAssembler_16aac7a84706:double,months_double_VectorAssembler_16aac7a84706:double>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>).\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:198)\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.SparkException: Encountered null while assembling a row with handleInvalid = \"error\". Consider\n",
      "removing nulls from dataset or using handleInvalid = \"keep\" or \"skip\".\n",
      "\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1(VectorAssembler.scala:291)\n",
      "\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1$adapted(VectorAssembler.scala:260)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n",
      "\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)\n",
      "\tat org.apache.spark.ml.feature.VectorAssembler$.assemble(VectorAssembler.scala:260)\n",
      "\tat org.apache.spark.ml.feature.VectorAssembler.$anonfun$transform$6(VectorAssembler.scala:143)\n",
      "\t... 20 more\n",
      "24/04/26 22:50:28 WARN TaskSetManager: Lost task 0.0 in stage 3.0 (TID 3) (10.255.255.254 executor driver): org.apache.spark.SparkException: [FAILED_EXECUTE_UDF] Failed to execute user defined function (`VectorAssembler$$Lambda$3228/0x000000084139d840`: (struct<age_double_VectorAssembler_16aac7a84706:double,sex_double_VectorAssembler_16aac7a84706:double,dayswfreq_double_VectorAssembler_16aac7a84706:double,tbilled:double,maccess:double,freeuse_double_VectorAssembler_16aac7a84706:double,nentries_double_VectorAssembler_16aac7a84706:double,cfreq_double_VectorAssembler_16aac7a84706:double,nrenewals_double_VectorAssembler_16aac7a84706:double,months_double_VectorAssembler_16aac7a84706:double>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>).\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:198)\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.SparkException: Encountered null while assembling a row with handleInvalid = \"error\". Consider\n",
      "removing nulls from dataset or using handleInvalid = \"keep\" or \"skip\".\n",
      "\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1(VectorAssembler.scala:291)\n",
      "\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1$adapted(VectorAssembler.scala:260)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n",
      "\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)\n",
      "\tat org.apache.spark.ml.feature.VectorAssembler$.assemble(VectorAssembler.scala:260)\n",
      "\tat org.apache.spark.ml.feature.VectorAssembler.$anonfun$transform$6(VectorAssembler.scala:143)\n",
      "\t... 20 more\n",
      "\n",
      "24/04/26 22:50:28 ERROR TaskSetManager: Task 0 in stage 3.0 failed 1 times; aborting job\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o73.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 3.0 failed 1 times, most recent failure: Lost task 0.0 in stage 3.0 (TID 3) (10.255.255.254 executor driver): org.apache.spark.SparkException: [FAILED_EXECUTE_UDF] Failed to execute user defined function (`VectorAssembler$$Lambda$3228/0x000000084139d840`: (struct<age_double_VectorAssembler_16aac7a84706:double,sex_double_VectorAssembler_16aac7a84706:double,dayswfreq_double_VectorAssembler_16aac7a84706:double,tbilled:double,maccess:double,freeuse_double_VectorAssembler_16aac7a84706:double,nentries_double_VectorAssembler_16aac7a84706:double,cfreq_double_VectorAssembler_16aac7a84706:double,nrenewals_double_VectorAssembler_16aac7a84706:double,months_double_VectorAssembler_16aac7a84706:double>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>).\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:198)\n\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.SparkException: Encountered null while assembling a row with handleInvalid = \"error\". Consider\nremoving nulls from dataset or using handleInvalid = \"keep\" or \"skip\".\n\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1(VectorAssembler.scala:291)\n\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1$adapted(VectorAssembler.scala:260)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)\n\tat org.apache.spark.ml.feature.VectorAssembler$.assemble(VectorAssembler.scala:260)\n\tat org.apache.spark.ml.feature.VectorAssembler.$anonfun$transform$6(VectorAssembler.scala:143)\n\t... 20 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:530)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4332)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3314)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4322)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4320)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4320)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3314)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3537)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:280)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:315)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.SparkException: [FAILED_EXECUTE_UDF] Failed to execute user defined function (`VectorAssembler$$Lambda$3228/0x000000084139d840`: (struct<age_double_VectorAssembler_16aac7a84706:double,sex_double_VectorAssembler_16aac7a84706:double,dayswfreq_double_VectorAssembler_16aac7a84706:double,tbilled:double,maccess:double,freeuse_double_VectorAssembler_16aac7a84706:double,nentries_double_VectorAssembler_16aac7a84706:double,cfreq_double_VectorAssembler_16aac7a84706:double,nrenewals_double_VectorAssembler_16aac7a84706:double,months_double_VectorAssembler_16aac7a84706:double>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>).\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:198)\n\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\nCaused by: org.apache.spark.SparkException: Encountered null while assembling a row with handleInvalid = \"error\". Consider\nremoving nulls from dataset or using handleInvalid = \"keep\" or \"skip\".\n\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1(VectorAssembler.scala:291)\n\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1$adapted(VectorAssembler.scala:260)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)\n\tat org.apache.spark.ml.feature.VectorAssembler$.assemble(VectorAssembler.scala:260)\n\tat org.apache.spark.ml.feature.VectorAssembler.$anonfun$transform$6(VectorAssembler.scala:143)\n\t... 20 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m col\n\u001b[1;32m      4\u001b[0m out\u001b[38;5;241m=\u001b[39massembler\u001b[38;5;241m.\u001b[39mtransform(df_customers)\n\u001b[0;32m----> 6\u001b[0m \u001b[43mout\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfeatures\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtruncate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/spark/lib/python3.12/site-packages/pyspark/sql/dataframe.py:945\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    885\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mshow\u001b[39m(\u001b[38;5;28mself\u001b[39m, n: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m, truncate: Union[\u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, vertical: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    886\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Prints the first ``n`` rows to the console.\u001b[39;00m\n\u001b[1;32m    887\u001b[0m \n\u001b[1;32m    888\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    943\u001b[0m \u001b[38;5;124;03m    name | Bob\u001b[39;00m\n\u001b[1;32m    944\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 945\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_show_string\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/anaconda3/envs/spark/lib/python3.12/site-packages/pyspark/sql/dataframe.py:976\u001b[0m, in \u001b[0;36mDataFrame._show_string\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    967\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[1;32m    968\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[1;32m    969\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_BOOL\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    970\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    973\u001b[0m         },\n\u001b[1;32m    974\u001b[0m     )\n\u001b[0;32m--> 976\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshowString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mint_truncate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/spark/lib/python3.12/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/anaconda3/envs/spark/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/anaconda3/envs/spark/lib/python3.12/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o73.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 3.0 failed 1 times, most recent failure: Lost task 0.0 in stage 3.0 (TID 3) (10.255.255.254 executor driver): org.apache.spark.SparkException: [FAILED_EXECUTE_UDF] Failed to execute user defined function (`VectorAssembler$$Lambda$3228/0x000000084139d840`: (struct<age_double_VectorAssembler_16aac7a84706:double,sex_double_VectorAssembler_16aac7a84706:double,dayswfreq_double_VectorAssembler_16aac7a84706:double,tbilled:double,maccess:double,freeuse_double_VectorAssembler_16aac7a84706:double,nentries_double_VectorAssembler_16aac7a84706:double,cfreq_double_VectorAssembler_16aac7a84706:double,nrenewals_double_VectorAssembler_16aac7a84706:double,months_double_VectorAssembler_16aac7a84706:double>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>).\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:198)\n\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.SparkException: Encountered null while assembling a row with handleInvalid = \"error\". Consider\nremoving nulls from dataset or using handleInvalid = \"keep\" or \"skip\".\n\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1(VectorAssembler.scala:291)\n\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1$adapted(VectorAssembler.scala:260)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)\n\tat org.apache.spark.ml.feature.VectorAssembler$.assemble(VectorAssembler.scala:260)\n\tat org.apache.spark.ml.feature.VectorAssembler.$anonfun$transform$6(VectorAssembler.scala:143)\n\t... 20 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:530)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4332)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3314)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4322)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4320)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4320)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3314)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3537)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:280)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:315)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.SparkException: [FAILED_EXECUTE_UDF] Failed to execute user defined function (`VectorAssembler$$Lambda$3228/0x000000084139d840`: (struct<age_double_VectorAssembler_16aac7a84706:double,sex_double_VectorAssembler_16aac7a84706:double,dayswfreq_double_VectorAssembler_16aac7a84706:double,tbilled:double,maccess:double,freeuse_double_VectorAssembler_16aac7a84706:double,nentries_double_VectorAssembler_16aac7a84706:double,cfreq_double_VectorAssembler_16aac7a84706:double,nrenewals_double_VectorAssembler_16aac7a84706:double,months_double_VectorAssembler_16aac7a84706:double>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>).\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:198)\n\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\nCaused by: org.apache.spark.SparkException: Encountered null while assembling a row with handleInvalid = \"error\". Consider\nremoving nulls from dataset or using handleInvalid = \"keep\" or \"skip\".\n\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1(VectorAssembler.scala:291)\n\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1$adapted(VectorAssembler.scala:260)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)\n\tat org.apache.spark.ml.feature.VectorAssembler$.assemble(VectorAssembler.scala:260)\n\tat org.apache.spark.ml.feature.VectorAssembler.$anonfun$transform$6(VectorAssembler.scala:143)\n\t... 20 more\n"
     ]
    }
   ],
   "source": [
    "# vamos visualizar o conteúdo com vector assembler\n",
    "# transform para criar um dataframe e aplicarmos o show\n",
    "from pyspark.sql.functions import col\n",
    "out=assembler.transform(df_customers)\n",
    "\n",
    "out.select(col(\"features\")).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "88b52c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eliminar nulls\n",
    "df_customers=df_customers.na.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8d85a9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the features and label columns\n",
    "feature_cols = [\"age\", \"sex\", \"dayswfreq\",\"tbilled\",\"maccess\",\"freeuse\",\"nentries\",\"cfreq\",\"nrenewals\",\"cref\",\"months\"]\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "data = assembler.transform(df_customers).select(\"features\", \"dropout\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "96198a1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------+\n",
      "|features                                              |\n",
      "+------------------------------------------------------+\n",
      "|[23.0,1.0,7.0,37.6,1.35,0.0,6.0,7.0,0.0,0.0,1.0]      |\n",
      "|[34.0,1.0,328.0,205.6,0.54,0.0,39.0,7.0,2.0,0.0,19.0] |\n",
      "|[24.0,0.0,3.0,140.0,0.8,0.0,28.0,7.0,0.0,0.0,8.0]     |\n",
      "|[20.0,1.0,41.0,71.6,1.0,0.0,13.0,7.0,0.0,0.0,3.0]     |\n",
      "|[21.0,1.0,18.0,113.2,0.08,0.0,7.0,7.0,3.0,0.0,24.0]   |\n",
      "|[20.0,0.0,38.0,118.6,0.33,0.0,11.0,7.0,2.0,0.0,10.0]  |\n",
      "|[26.0,1.0,279.0,53.2,0.16,0.0,6.0,7.0,1.0,0.0,9.0]    |\n",
      "|[22.0,0.0,292.0,73.7,0.63,0.0,29.0,7.0,1.0,0.0,11.0]  |\n",
      "|[32.0,1.0,204.0,240.5,0.11,0.0,11.0,7.0,2.0,0.0,25.0] |\n",
      "|[20.0,1.0,21.0,95.6,0.33,0.0,8.0,7.0,0.0,0.0,6.0]     |\n",
      "|[50.0,0.0,15.0,1238.4,1.22,0.0,163.0,7.0,3.0,0.0,34.0]|\n",
      "|[21.0,0.0,133.0,73.6,0.62,0.0,15.0,7.0,1.0,0.0,6.0]   |\n",
      "|[43.0,1.0,1.0,76.6,0.34,0.0,5.0,7.0,0.0,0.0,4.0]      |\n",
      "|[23.0,0.0,25.0,45.1,0.04,0.0,1.0,7.0,0.0,0.0,6.0]     |\n",
      "|[41.0,0.0,7.0,614.2,2.69,0.0,147.0,7.0,1.0,0.0,14.0]  |\n",
      "|[23.0,1.0,13.0,93.6,0.6,0.0,13.0,7.0,0.0,0.0,5.0]     |\n",
      "|[29.0,1.0,31.0,101.6,1.34,0.0,29.0,7.0,0.0,0.0,5.0]   |\n",
      "|[49.0,0.0,47.0,233.6,0.79,0.0,24.0,4.0,1.0,0.0,8.0]   |\n",
      "|[35.0,0.0,51.0,218.6,0.68,0.0,20.0,4.0,1.0,0.0,8.0]   |\n",
      "|[20.0,0.0,13.0,82.6,0.61,0.0,14.0,7.0,1.0,0.0,7.0]    |\n",
      "+------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# vamos visualizar o conteúdo com vector assembler\n",
    "# transform para criar um dataframe e aplicarmos o show\n",
    "from pyspark.sql.functions import col\n",
    "out=assembler.transform(df_customers)\n",
    "\n",
    "out.select(\"features\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2c31afbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "train, test = out.randomSplit([0.5, 0.5],seed=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cc1b2a0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+---------+-------+-------+-------+--------+-----+---------+----+---------+------+-------+--------+\n",
      "| Id|age|sex|dayswfreq|tbilled|maccess|freeuse|nentries|cfreq|nrenewals|cref|startDate|months|dropout|features|\n",
      "+---+---+---+---------+-------+-------+-------+--------+-----+---------+----+---------+------+-------+--------+\n",
      "| 00| 23|  1|        7|     37|     1.|      0|       6|    7|        0|   0|       20|     1|      1|      [2|\n",
      "| 00| 20|  1|       41|     71|     1.|      0|      13|    7|        0|   0|       20|     3|      1|      [2|\n",
      "| 00| 21|  1|       18|     11|     0.|      0|       7|    7|        3|   0|       20|    24|      1|      [2|\n",
      "| 00| 20|  0|       38|     11|     0.|      0|      11|    7|        2|   0|       20|    10|      1|      [2|\n",
      "| 01| 50|  0|       15|     12|     1.|      0|      16|    7|        3|   0|       20|    34|      0|      [5|\n",
      "| 01| 21|  0|       13|     73|     0.|      0|      15|    7|        1|   0|       20|     6|      1|      [2|\n",
      "| 01| 29|  1|       31|     10|     1.|      0|      29|    7|        0|   0|       20|     5|      1|      [2|\n",
      "| 01| 49|  0|       47|     23|     0.|      0|      24|    4|        1|   0|       20|     8|      1|      [4|\n",
      "| 01| 20|  0|       13|     82|     0.|      0|      14|    7|        1|   0|       20|     7|      1|      [2|\n",
      "| 01| 39|  0|       53|     61|     1.|      0|      19|    7|        0|   0|       20|     3|      1|      [3|\n",
      "| 02| 20|  1|       12|     33|     0.|      0|       2|    7|        1|   0|       20|     5|      1|      [2|\n",
      "| 02| 21|  1|        5|     30|     0.|      0|      43|    7|        2|   0|       20|    26|      1|      [2|\n",
      "| 02| 44|  1|       16|     61|     0.|      0|       8|    7|        1|   0|       20|     6|      1|      [4|\n",
      "| 02| 21|  0|       91|     16|     0.|      0|      28|    7|        2|   0|       20|    17|      1|      [2|\n",
      "| 02| 21|  1|       39|     12|     0.|      0|      18|    7|        1|   0|       20|     9|      1|      [2|\n",
      "| 02| 24|  0|       33|     44|     1.|      0|      12|    7|        1|   0|       20|    22|      1|      [2|\n",
      "| 02| 26|  0|       54|     43|     0.|      0|       9|    7|        0|   0|       20|     2|      1|      [2|\n",
      "| 02| 20|  0|       56|     47|     0.|      0|       2|    7|        0|   0|       20|     2|      1|      [2|\n",
      "| 02| 69|  0|       88|     16|     0.|      0|      14|    7|        0|   0|       20|     5|      1|      [6|\n",
      "| 02| 31|  1|       38|     19|     1.|      0|      48|    7|        0|   0|       20|     8|      1|      [3|\n",
      "+---+---+---+---------+-------+-------+-------+--------+-----+---------+----+---------+------+-------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train.show(truncate=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a03ec262",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 6:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+-----+-----+---------+-------+-------+-------+--------+-----+---------+-----+------+-------+\n",
      "|summary|   Id|  age|  sex|dayswfreq|tbilled|maccess|freeuse|nentries|cfreq|nrenewals| cref|months|dropout|\n",
      "+-------+-----+-----+-----+---------+-------+-------+-------+--------+-----+---------+-----+------+-------+\n",
      "|  count| 2598| 2598| 2598|     2598|   2598|   2598|   2598|    2598| 2598|     2598| 2598|  2598|   2598|\n",
      "|   mean| null|27...|0....|    75...|  15...|  0....|  0....|   29...|6....|    0....|0....| 9....|  0....|\n",
      "|  st...| null|11...|0....|    99...|  16...|  0....|  0....|   42...|0....|    0....|0....| 8....|  0....|\n",
      "|    min|00...|   15|    0|        0|   23.6|   0.01|      0|       1|    2|        0|    0|     1|      0|\n",
      "|    max|FF...|   93|    1|      916|  19...|  10.33|      1|     563|    7|        4|    2|    47|      1|\n",
      "+-------+-----+-----+-----+---------+-------+-------+-------+--------+-----+---------+-----+------+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "train.describe().show(truncate=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "940b3629",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+-----+-----+---------+-------+-------+-------+--------+-----+---------+-----+------+-------+\n",
      "|summary|   Id|  age|  sex|dayswfreq|tbilled|maccess|freeuse|nentries|cfreq|nrenewals| cref|months|dropout|\n",
      "+-------+-----+-----+-----+---------+-------+-------+-------+--------+-----+---------+-----+------+-------+\n",
      "|  count| 2607| 2607| 2607|     2607|   2607|   2607|   2607|    2607| 2607|     2607| 2607|  2607|   2607|\n",
      "|   mean| null|27...|0....|    77...|  15...|  0....|  0....|   28...|6....|    0....|0....| 9....|  0....|\n",
      "|  st...| null|11...|0....|    10...|  16...|  0....|  0....|   39...|0....|    0....|0....| 7....|  0....|\n",
      "|    min|00...|    0|    0|        0|    3.6|   0.01|      0|       1|    2|        0|    0|     0|      0|\n",
      "|    max|FF...|   83|    1|      991|  37...|   5.81|      1|     585|    7|        4|    1|    47|      1|\n",
      "+-------+-----+-----+-----+---------+-------+-------+-------+--------+-----+---------+-----+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test.describe().show(truncate=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6875f85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "dt = DecisionTreeClassifier(labelCol=\"dropout\", featuresCol=\"features\")\n",
    "model = dt.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "249ca6d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9125431530494822\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Evaluate model\n",
    "predictions = model.transform(test)\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"dropout\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "38b65736",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+---+---------+-------+-------+-------+--------+-----+---------+----+---------+------+-------+--------+-------------+-----------+----------+\n",
      "|      Id|age|sex|dayswfreq|tbilled|maccess|freeuse|nentries|cfreq|nrenewals|cref|startDate|months|dropout|features|rawPrediction|probability|prediction|\n",
      "+--------+---+---+---------+-------+-------+-------+--------+-----+---------+----+---------+------+-------+--------+-------------+-----------+----------+\n",
      "|0016D...| 34|  1|      328|  205.6|   0.54|      0|      39|    7|        2|   0| 2016-...|    19|      0|[34.0...|     [23.0...|   [0.01...|       1.0|\n",
      "|001A7...| 24|  0|        3|  140.0|    0.8|      0|      28|    7|        0|   0| 2015-...|     8|      1|[24.0...|     [4.0,...|   [0.03...|       1.0|\n",
      "|004D1...| 26|  1|      279|   53.2|   0.16|      0|       6|    7|        1|   0| 2014-...|     9|      1|[26.0...|     [23.0...|   [0.01...|       1.0|\n",
      "|00CE4...| 22|  0|      292|   73.7|   0.63|      0|      29|    7|        1|   0| 2014-...|    11|      1|[22.0...|     [23.0...|   [0.01...|       1.0|\n",
      "|00F65...| 32|  1|      204|  240.5|   0.11|      0|      11|    7|        2|   0| 2015-...|    25|      0|[32.0...|     [14.0...|   [0.09...|       1.0|\n",
      "|01108...| 20|  1|       21|   95.6|   0.33|      0|       8|    7|        0|   0| 2016-...|     6|      1|[20.0...|     [23.0...|   [0.01...|       1.0|\n",
      "|01148...| 43|  1|        1|   76.6|   0.34|      0|       5|    7|        0|   0| 2016-...|     4|      1|[43.0...|     [4.0,...|   [0.03...|       1.0|\n",
      "|01173...| 23|  0|       25|   45.1|   0.04|      0|       1|    7|        0|   0| 2016-...|     6|      1|[23.0...|     [23.0...|   [0.01...|       1.0|\n",
      "|011BB...| 41|  0|        7|  614.2|   2.69|      0|     147|    7|        1|   0| 2016-...|    14|      0|[41.0...|     [2.0,...|   [1.0,...|       0.0|\n",
      "|01505...| 23|  1|       13|   93.6|    0.6|      0|      13|    7|        0|   0| 2016-...|     5|      1|[23.0...|     [23.0...|   [0.01...|       1.0|\n",
      "|016BB...| 35|  0|       51|  218.6|   0.68|      0|      20|    4|        1|   0| 2015-...|     8|      1|[35.0...|     [23.0...|   [0.01...|       1.0|\n",
      "|018BC...| 19|  0|        4|   36.1|   1.55|      0|       2|    7|        0|   0| 2017-...|     1|      0|[19.0...|     [17.0...|   [0.24...|       1.0|\n",
      "|01EA5...| 22|  0|       38|   68.0|   0.54|      0|       8|    7|        0|   0| 2015-...|     4|      1|[22.0...|     [23.0...|   [0.01...|       1.0|\n",
      "|0209D...| 32|  1|      122|  198.2|   0.26|      0|      13|    7|        1|   0| 2014-...|    12|      1|[32.0...|     [23.0...|   [0.01...|       1.0|\n",
      "|022D3...| 20|  1|      103| 114.85|   0.83|      0|      21|    7|        1|   0| 2015-...|     7|      1|[20.0...|     [23.0...|   [0.01...|       1.0|\n",
      "|02434...| 19|  0|      153|   43.6|   0.06|      0|       1|    7|        1|   0| 2015-...|     5|      1|[19.0...|     [23.0...|   [0.01...|       1.0|\n",
      "|02443...| 21|  1|      286|   71.2|   0.24|      0|      10|    7|        1|   0| 2014-...|    11|      1|[21.0...|     [23.0...|   [0.01...|       1.0|\n",
      "|028C1...| 19|  1|       15|  197.2|   0.38|      0|      20|    7|        1|   0| 2015-...|    13|      1|[19.0...|     [23.0...|   [0.01...|       1.0|\n",
      "|02990...| 23|  0|       13|  222.6|   1.38|      0|      48|    7|        0|   0| 2016-...|     8|      1|[23.0...|     [23.0...|   [0.01...|       1.0|\n",
      "|029EC...| 28|  0|       22|   88.6|   0.54|      0|       2|    7|        0|   0| 2017-...|     3|      1|[28.0...|     [23.0...|   [0.01...|       1.0|\n",
      "+--------+---+---+---------+-------+-------+-------+--------+-----+---------+----+---------+------+-------+--------+-------------+-----------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.show(20,truncate=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "73ae961d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df=predictions.toPandas()\n",
    "\n",
    "df.to_csv(\"ola.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9cab84dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassificationModel: uid=DecisionTreeClassifier_7df03b066e8a, depth=5, numNodes=49, numClasses=2, numFeatures=11\n",
      "  If (feature 2 <= 6.5)\n",
      "   If (feature 8 <= 0.5)\n",
      "    If (feature 10 <= 2.5)\n",
      "     If (feature 2 <= 1.5)\n",
      "      If (feature 4 <= 1.1949999999999998)\n",
      "       Predict: 1.0\n",
      "      Else (feature 4 > 1.1949999999999998)\n",
      "       Predict: 0.0\n",
      "     Else (feature 2 > 1.5)\n",
      "      If (feature 3 <= 103.55)\n",
      "       Predict: 1.0\n",
      "      Else (feature 3 > 103.55)\n",
      "       Predict: 0.0\n",
      "    Else (feature 10 > 2.5)\n",
      "     If (feature 5 <= 0.5)\n",
      "      If (feature 7 <= 5.0)\n",
      "       Predict: 0.0\n",
      "      Else (feature 7 > 5.0)\n",
      "       Predict: 1.0\n",
      "     Else (feature 5 > 0.5)\n",
      "      If (feature 6 <= 25.5)\n",
      "       Predict: 0.0\n",
      "      Else (feature 6 > 25.5)\n",
      "       Predict: 1.0\n",
      "   Else (feature 8 > 0.5)\n",
      "    If (feature 0 <= 38.5)\n",
      "     If (feature 2 <= 1.5)\n",
      "      Predict: 0.0\n",
      "     Else (feature 2 > 1.5)\n",
      "      Predict: 1.0\n",
      "    Else (feature 0 > 38.5)\n",
      "     If (feature 2 <= 5.5)\n",
      "      Predict: 0.0\n",
      "     Else (feature 2 > 5.5)\n",
      "      If (feature 6 <= 25.5)\n",
      "       Predict: 1.0\n",
      "      Else (feature 6 > 25.5)\n",
      "       Predict: 0.0\n",
      "  Else (feature 2 > 6.5)\n",
      "   If (feature 3 <= 563.6)\n",
      "    If (feature 10 <= 19.5)\n",
      "     If (feature 0 <= 61.5)\n",
      "      Predict: 1.0\n",
      "     Else (feature 0 > 61.5)\n",
      "      If (feature 10 <= 11.5)\n",
      "       Predict: 1.0\n",
      "      Else (feature 10 > 11.5)\n",
      "       Predict: 0.0\n",
      "    Else (feature 10 > 19.5)\n",
      "     If (feature 2 <= 9.5)\n",
      "      If (feature 5 <= 0.5)\n",
      "       Predict: 1.0\n",
      "      Else (feature 5 > 0.5)\n",
      "       Predict: 0.0\n",
      "     Else (feature 2 > 9.5)\n",
      "      Predict: 1.0\n",
      "   Else (feature 3 > 563.6)\n",
      "    If (feature 4 <= 1.125)\n",
      "     If (feature 2 <= 17.5)\n",
      "      Predict: 0.0\n",
      "     Else (feature 2 > 17.5)\n",
      "      If (feature 0 <= 38.5)\n",
      "       Predict: 0.0\n",
      "      Else (feature 0 > 38.5)\n",
      "       Predict: 1.0\n",
      "    Else (feature 4 > 1.125)\n",
      "     If (feature 0 <= 31.5)\n",
      "      Predict: 1.0\n",
      "     Else (feature 0 > 31.5)\n",
      "      If (feature 4 <= 1.4049999999999998)\n",
      "       Predict: 1.0\n",
      "      Else (feature 4 > 1.4049999999999998)\n",
      "       Predict: 0.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(model.toDebugString)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "366d3265",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_string = model.toDebugString"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8ba43030",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'DecisionTreeClassificationModel: uid=DecisionTreeClassifier_9431e4032b52, depth=5, numNodes=51, numClasses=2, numFeatures=11\\n  If (feature 2 <= 7.5)\\n   If (feature 8 <= 0.5)\\n    If (feature 10 <= 2.5)\\n     If (feature 2 <= 1.5)\\n      If (feature 4 <= 1.395)\\n       Predict: 1.0\\n      Else (feature 4 > 1.395)\\n       Predict: 0.0\\n     Else (feature 2 > 1.5)\\n      If (feature 7 <= 3.0)\\n       Predict: 0.0\\n      Else (feature 7 > 3.0)\\n       Predict: 1.0\\n    Else (feature 10 > 2.5)\\n     If (feature 5 <= 0.5)\\n      Predict: 1.0\\n     Else (feature 5 > 0.5)\\n      If (feature 6 <= 24.5)\\n       Predict: 0.0\\n      Else (feature 6 > 24.5)\\n       Predict: 1.0\\n   Else (feature 8 > 0.5)\\n    If (feature 0 <= 32.5)\\n     If (feature 5 <= 0.5)\\n      If (feature 3 <= 547.1700000000001)\\n       Predict: 1.0\\n      Else (feature 3 > 547.1700000000001)\\n       Predict: 0.0\\n     Else (feature 5 > 0.5)\\n      If (feature 10 <= 19.5)\\n       Predict: 1.0\\n      Else (feature 10 > 19.5)\\n       Predict: 0.0\\n    Else (feature 0 > 32.5)\\n     If (feature 3 <= 86.82499999999999)\\n      Predict: 1.0\\n     Else (feature 3 > 86.82499999999999)\\n      Predict: 0.0\\n  Else (feature 2 > 7.5)\\n   If (feature 10 <= 23.5)\\n    If (feature 0 <= 61.5)\\n     If (feature 3 <= 547.1700000000001)\\n      Predict: 1.0\\n     Else (feature 3 > 547.1700000000001)\\n      If (feature 10 <= 15.5)\\n       Predict: 0.0\\n      Else (feature 10 > 15.5)\\n       Predict: 1.0\\n    Else (feature 0 > 61.5)\\n     If (feature 6 <= 4.5)\\n      If (feature 3 <= 44.475)\\n       Predict: 0.0\\n      Else (feature 3 > 44.475)\\n       Predict: 1.0\\n     Else (feature 6 > 4.5)\\n      If (feature 4 <= 0.155)\\n       Predict: 0.0\\n      Else (feature 4 > 0.155)\\n       Predict: 1.0\\n   Else (feature 10 > 23.5)\\n    If (feature 3 <= 547.1700000000001)\\n     If (feature 4 <= 1.2850000000000001)\\n      Predict: 1.0\\n     Else (feature 4 > 1.2850000000000001)\\n      If (feature 1 <= 0.5)\\n       Predict: 1.0\\n      Else (feature 1 > 0.5)\\n       Predict: 0.0\\n    Else (feature 3 > 547.1700000000001)\\n     If (feature 8 <= 3.5)\\n      If (feature 4 <= 1.025)\\n       Predict: 0.0\\n      Else (feature 4 > 1.025)\\n       Predict: 1.0\\n     Else (feature 8 > 3.5)\\n      Predict: 1.0\\n'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_string"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ec9ad03d",
   "metadata": {},
   "source": [
    "# completar ... visualizar a tree\n",
    "\n",
    "* melhorar para ter o nome das features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c0c4081f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'DecisionTreeClassificationModel: uid=DecisionTreeClassifier_9431e4032b52, depth=5, numNodes=51, numClasses=2, numFeatures=11\\n  If (feature 2 <= 7.5)\\n   If (feature 8 <= 0.5)\\n    If (feature 10 <= 2.5)\\n     If (feature 2 <= 1.5)\\n      If (feature 4 <= 1.395)\\n       Predict: 1.0\\n      Else (feature 4 > 1.395)\\n       Predict: 0.0\\n     Else (feature 2 > 1.5)\\n      If (feature 7 <= 3.0)\\n       Predict: 0.0\\n      Else (feature 7 > 3.0)\\n       Predict: 1.0\\n    Else (feature 10 > 2.5)\\n     If (feature 5 <= 0.5)\\n      Predict: 1.0\\n     Else (feature 5 > 0.5)\\n      If (feature 6 <= 24.5)\\n       Predict: 0.0\\n      Else (feature 6 > 24.5)\\n       Predict: 1.0\\n   Else (feature 8 > 0.5)\\n    If (feature 0 <= 32.5)\\n     If (feature 5 <= 0.5)\\n      If (feature 3 <= 547.1700000000001)\\n       Predict: 1.0\\n      Else (feature 3 > 547.1700000000001)\\n       Predict: 0.0\\n     Else (feature 5 > 0.5)\\n      If (feature 10 <= 19.5)\\n       Predict: 1.0\\n      Else (feature 10 > 19.5)\\n       Predict: 0.0\\n    Else (feature 0 > 32.5)\\n     If (feature 3 <= 86.82499999999999)\\n      Predict: 1.0\\n     Else (feature 3 > 86.82499999999999)\\n      Predict: 0.0\\n  Else (feature 2 > 7.5)\\n   If (feature 10 <= 23.5)\\n    If (feature 0 <= 61.5)\\n     If (feature 3 <= 547.1700000000001)\\n      Predict: 1.0\\n     Else (feature 3 > 547.1700000000001)\\n      If (feature 10 <= 15.5)\\n       Predict: 0.0\\n      Else (feature 10 > 15.5)\\n       Predict: 1.0\\n    Else (feature 0 > 61.5)\\n     If (feature 6 <= 4.5)\\n      If (feature 3 <= 44.475)\\n       Predict: 0.0\\n      Else (feature 3 > 44.475)\\n       Predict: 1.0\\n     Else (feature 6 > 4.5)\\n      If (feature 4 <= 0.155)\\n       Predict: 0.0\\n      Else (feature 4 > 0.155)\\n       Predict: 1.0\\n   Else (feature 10 > 23.5)\\n    If (feature 3 <= 547.1700000000001)\\n     If (feature 4 <= 1.2850000000000001)\\n      Predict: 1.0\\n     Else (feature 4 > 1.2850000000000001)\\n      If (feature 1 <= 0.5)\\n       Predict: 1.0\\n      Else (feature 1 > 0.5)\\n       Predict: 0.0\\n    Else (feature 3 > 547.1700000000001)\\n     If (feature 8 <= 3.5)\\n      If (feature 4 <= 1.025)\\n       Predict: 0.0\\n      Else (feature 4 > 1.025)\\n       Predict: 1.0\\n     Else (feature 8 > 3.5)\\n      Predict: 1.0\\n'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model._call_java(\"toDebugString\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cd8449f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_string = model._call_java(\"toDebugString\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d24f55f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassificationModel: uid=DecisionTreeClassifier_9431e4032b52, depth=5, numNodes=51, numClasses=2, numFeatures=11\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6541f681",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the tree string and create a graphviz object\n",
    "dot = Digraph(comment=\"Decision Tree\")\n",
    "for line in tree_string.split(\"\\n\"):\n",
    "    if line.strip():\n",
    "        if \"If\" in line:\n",
    "            node_id = line.split(\" \")[1]\n",
    "            feature = line.split(\" \")[3].replace(\"(\", \"\").replace(\",\", \"\")\n",
    "            threshold = line.split(\" \")[4].replace(\")\", \"\")\n",
    "            dot.node(node_id, f\"{feature} <= {threshold}\")\n",
    "        elif \"Predict\" in line:\n",
    "            node_id = line.split(\":\")[0]\n",
    "            prediction = line.split(\":\")[1].strip()\n",
    "            dot.node(node_id, f\"Prediction: {prediction}\")\n",
    "        elif \"|\" in line:\n",
    "            prev_node_id = line.split(\"|\")[1].strip()\n",
    "            node_id = line.split(\"|\")[0].strip()\n",
    "            dot.edge(prev_node_id, node_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ac400e35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'decision_tree.pdf'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the decision tree visualization to a file\n",
    "dot.render(\"decision_tree\", format=\"pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "156c2339",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassificationModel: uid=DecisionTreeClassifier_9431e4032b52, depth=5, numNodes=51, numClasses=2, numFeatures=11\n",
      "^\n",
      "Expected {'GRAPH' | 'DIGRAPH'}, found 'DecisionTreeClas'  (at char 0), (line:1, col:1)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_25327/2459366397.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtree_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdot_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtree_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoDebugString\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mgraph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpydot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_from_dot_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdot_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mImage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_png\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "import pydot\n",
    "tree_model = model\n",
    "dot_data = tree_model.toDebugString\n",
    "graph = pydot.graph_from_dot_data(dot_data)[0]\n",
    "display(Image(graph.create_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "b02235d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error: tree: syntax error in line 1 near 'DecisionTreeClassificationModel'\n"
     ]
    },
    {
     "ename": "CalledProcessError",
     "evalue": "Command '[PosixPath('dot'), '-Kdot', '-Tpng', '-O', 'tree']' returned non-zero exit status 1. [stderr: b\"Error: tree: syntax error in line 1 near 'DecisionTreeClassificationModel'\\n\"]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m~/miniconda3/envs/trading/lib/python3.7/site-packages/graphviz/backend/execute.py\u001b[0m in \u001b[0;36mrun_check\u001b[0;34m(cmd, input_lines, encoding, quiet, **kwargs)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m         \u001b[0mproc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_returncode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0msubprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCalledProcessError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/trading/lib/python3.7/subprocess.py\u001b[0m in \u001b[0;36mcheck_returncode\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    443\u001b[0m             raise CalledProcessError(self.returncode, self.args, self.stdout,\n\u001b[0;32m--> 444\u001b[0;31m                                      self.stderr)\n\u001b[0m\u001b[1;32m    445\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mCalledProcessError\u001b[0m: Command '[PosixPath('dot'), '-Kdot', '-Tpng', '-O', 'tree']' returned non-zero exit status 1.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_21388/1130417683.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mgraph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraphviz\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tree.dot'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'png'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tree'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Display image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/trading/lib/python3.7/site-packages/graphviz/_tools.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    169\u001b[0m                               category=category)\n\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/trading/lib/python3.7/site-packages/graphviz/rendering.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, filename, directory, view, cleanup, format, renderer, formatter, neato_no_op, quiet, quiet_view, outfile, engine, raise_if_result_exists, overwrite_source)\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m         \u001b[0mrendered\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcleanup\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/trading/lib/python3.7/site-packages/graphviz/_tools.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    169\u001b[0m                               category=category)\n\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/trading/lib/python3.7/site-packages/graphviz/backend/rendering.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(engine, format, filepath, renderer, formatter, neato_no_op, quiet, outfile, raise_if_result_exists, overwrite_filepath)\u001b[0m\n\u001b[1;32m    325\u001b[0m                       \u001b[0mcwd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparent\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparts\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m                       \u001b[0mquiet\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquiet\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 327\u001b[0;31m                       capture_output=True)\n\u001b[0m\u001b[1;32m    328\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/trading/lib/python3.7/site-packages/graphviz/backend/execute.py\u001b[0m in \u001b[0;36mrun_check\u001b[0;34m(cmd, input_lines, encoding, quiet, **kwargs)\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0mproc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_returncode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0msubprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCalledProcessError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mCalledProcessError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mproc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mCalledProcessError\u001b[0m: Command '[PosixPath('dot'), '-Kdot', '-Tpng', '-O', 'tree']' returned non-zero exit status 1. [stderr: b\"Error: tree: syntax error in line 1 near 'DecisionTreeClassificationModel'\\n\"]"
     ]
    }
   ],
   "source": [
    "import graphviz\n",
    "from IPython.display import Image\n",
    "\n",
    "# Convert .dot file to .png image\n",
    "graph = graphviz.Source.from_file('tree.dot')\n",
    "graph.format = 'png'\n",
    "graph.render('tree')\n",
    "\n",
    "# Display image\n",
    "Image(filename='tree.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2800ae2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'DecisionTreeClassificationModel: uid=DecisionTreeClassifier_9431e4032b52, depth=5, numNodes=51, numClasses=2, numFeatures=11\\n  If (feature 2 <= 7.5)\\n   If (feature 8 <= 0.5)\\n    If (feature 10 <= 2.5)\\n     If (feature 2 <= 1.5)\\n      If (feature 4 <= 1.395)\\n       Predict: 1.0\\n      Else (feature 4 > 1.395)\\n       Predict: 0.0\\n     Else (feature 2 > 1.5)\\n      If (feature 7 <= 3.0)\\n       Predict: 0.0\\n      Else (feature 7 > 3.0)\\n       Predict: 1.0\\n    Else (feature 10 > 2.5)\\n     If (feature 5 <= 0.5)\\n      Predict: 1.0\\n     Else (feature 5 > 0.5)\\n      If (feature 6 <= 24.5)\\n       Predict: 0.0\\n      Else (feature 6 > 24.5)\\n       Predict: 1.0\\n   Else (feature 8 > 0.5)\\n    If (feature 0 <= 32.5)\\n     If (feature 5 <= 0.5)\\n      If (feature 3 <= 547.1700000000001)\\n       Predict: 1.0\\n      Else (feature 3 > 547.1700000000001)\\n       Predict: 0.0\\n     Else (feature 5 > 0.5)\\n      If (feature 10 <= 19.5)\\n       Predict: 1.0\\n      Else (feature 10 > 19.5)\\n       Predict: 0.0\\n    Else (feature 0 > 32.5)\\n     If (feature 3 <= 86.82499999999999)\\n      Predict: 1.0\\n     Else (feature 3 > 86.82499999999999)\\n      Predict: 0.0\\n  Else (feature 2 > 7.5)\\n   If (feature 10 <= 23.5)\\n    If (feature 0 <= 61.5)\\n     If (feature 3 <= 547.1700000000001)\\n      Predict: 1.0\\n     Else (feature 3 > 547.1700000000001)\\n      If (feature 10 <= 15.5)\\n       Predict: 0.0\\n      Else (feature 10 > 15.5)\\n       Predict: 1.0\\n    Else (feature 0 > 61.5)\\n     If (feature 6 <= 4.5)\\n      If (feature 3 <= 44.475)\\n       Predict: 0.0\\n      Else (feature 3 > 44.475)\\n       Predict: 1.0\\n     Else (feature 6 > 4.5)\\n      If (feature 4 <= 0.155)\\n       Predict: 0.0\\n      Else (feature 4 > 0.155)\\n       Predict: 1.0\\n   Else (feature 10 > 23.5)\\n    If (feature 3 <= 547.1700000000001)\\n     If (feature 4 <= 1.2850000000000001)\\n      Predict: 1.0\\n     Else (feature 4 > 1.2850000000000001)\\n      If (feature 1 <= 0.5)\\n       Predict: 1.0\\n      Else (feature 1 > 0.5)\\n       Predict: 0.0\\n    Else (feature 3 > 547.1700000000001)\\n     If (feature 8 <= 3.5)\\n      If (feature 4 <= 1.025)\\n       Predict: 0.0\\n      Else (feature 4 > 1.025)\\n       Predict: 1.0\\n     Else (feature 8 > 3.5)\\n      Predict: 1.0\\n'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6668143b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'decision_tree.pdf'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the string representation of the decision tree\n",
    "tree_string = model._call_java(\"toDebugString\")\n",
    "\n",
    "# Parse the tree string and create a graphviz object\n",
    "dot = Digraph(comment=\"Decision Tree\")\n",
    "for line in tree_string.split(\"\\n\"):\n",
    "    if line.strip():\n",
    "        if \"If\" in line:\n",
    "            node_id = line.split(\" \")[1]\n",
    "            feature = line.split(\" \")[3].replace(\"(\", \"\").replace(\",\", \"\")\n",
    "            threshold = line.split(\" \")[4].replace(\")\", \"\")\n",
    "            dot.node(node_id, f\"{feature} <= {threshold}\")\n",
    "        elif \"Predict\" in line:\n",
    "            node_id = line.split(\":\")[0]\n",
    "            prediction = line.split(\":\")[1].strip()\n",
    "            dot.node(node_id, f\"Prediction: {prediction}\")\n",
    "        elif \"|\" in line:\n",
    "            prev_node_id = line.split(\"|\")[1].strip()\n",
    "            node_id = line.split(\"|\")[0].strip()\n",
    "            dot.edge(prev_node_id, node_id)\n",
    "\n",
    "# Save the decision tree visualization to a file\n",
    "dot.render(\"decision_tree\", format=\"pdf\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29034289",
   "metadata": {},
   "source": [
    "# Matrix de correlação\n",
    "\n",
    "* temos que converter para um vector com 1 dimensão\n",
    "* temos um dataframe dados heterogeneous e queremos converter para um array homogeneo\n",
    "* Most machine learning algorithms are designed to work with numerical vectors\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea664dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.stat import Correlation\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f37d507a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_customers' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mdf_customers\u001b[49m[features]\u001b[38;5;241m.\u001b[39mshow(\u001b[38;5;241m5\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df_customers' is not defined"
     ]
    }
   ],
   "source": [
    "df_customers[features].show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c18ab1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---------+-------+-------+-------+--------+-----+---------+----+------+-------+\n",
      "|age|sex|dayswfreq|tbilled|maccess|freeuse|nentries|cfreq|nrenewals|cref|months|dropout|\n",
      "+---+---+---------+-------+-------+-------+--------+-----+---------+----+------+-------+\n",
      "|  0|  0|        0|      0|      0|      0|       0|    0|        0|   0|     0|      0|\n",
      "+---+---+---------+-------+-------+-------+--------+-----+---------+----+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check null values\n",
    "df_customers.select([F.count(F.when(F.isnan(c) | F.col(c).isNull(), c)).alias(c) for c in features]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2754b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop nulls values\n",
    "df_customers = df_customers.dropna(subset=features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70228bc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---+---+---------+-------+-------+-------+--------+-----+---------+----+--------------------+------+-------+\n",
      "|                  Id|age|sex|dayswfreq|tbilled|maccess|freeuse|nentries|cfreq|nrenewals|cref|           startDate|months|dropout|\n",
      "+--------------------+---+---+---------+-------+-------+-------+--------+-----+---------+----+--------------------+------+-------+\n",
      "|00130FE8-5D34-4C6...| 23|  1|        7|   37.6|   1.35|      0|       6|    7|        0|   0|2017-05-02 09:43:...|     1|      1|\n",
      "|0016D0DD-E713-4ED...| 34|  1|      328|  205.6|   0.54|      0|      39|    7|        2|   0|2016-04-20 18:33:...|    19|      0|\n",
      "|001A70CA-0985-42D...| 24|  0|        3|  140.0|    0.8|      0|      28|    7|        0|   0|2015-12-01 19:47:...|     8|      1|\n",
      "|001B9AC2-7711-438...| 20|  1|       41|   71.6|    1.0|      0|      13|    7|        0|   0|2015-10-06 16:33:...|     3|      1|\n",
      "|001D927E-ACD2-4B3...| 21|  1|       18|  113.2|   0.08|      0|       7|    7|        3|   0|2015-03-25 15:25:...|    24|      1|\n",
      "+--------------------+---+---+---------+-------+-------+-------+--------+-----+---------+----+--------------------+------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_customers.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe29521a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---+---+---------+-------+-------+-------+--------+-----+---------+----+--------------------+------+-------+--------------------+\n",
      "|                  Id|age|sex|dayswfreq|tbilled|maccess|freeuse|nentries|cfreq|nrenewals|cref|           startDate|months|dropout|            features|\n",
      "+--------------------+---+---+---------+-------+-------+-------+--------+-----+---------+----+--------------------+------+-------+--------------------+\n",
      "|00130FE8-5D34-4C6...| 23|  1|        7|   37.6|   1.35|      0|       6|    7|        0|   0|2017-05-02 09:43:...|     1|      1|[23.0,1.0,7.0,37....|\n",
      "|0016D0DD-E713-4ED...| 34|  1|      328|  205.6|   0.54|      0|      39|    7|        2|   0|2016-04-20 18:33:...|    19|      0|[34.0,1.0,328.0,2...|\n",
      "|001A70CA-0985-42D...| 24|  0|        3|  140.0|    0.8|      0|      28|    7|        0|   0|2015-12-01 19:47:...|     8|      1|[24.0,0.0,3.0,140...|\n",
      "|001B9AC2-7711-438...| 20|  1|       41|   71.6|    1.0|      0|      13|    7|        0|   0|2015-10-06 16:33:...|     3|      1|[20.0,1.0,41.0,71...|\n",
      "|001D927E-ACD2-4B3...| 21|  1|       18|  113.2|   0.08|      0|       7|    7|        3|   0|2015-03-25 15:25:...|    24|      1|[21.0,1.0,18.0,11...|\n",
      "|00392117-8F5B-46E...| 20|  0|       38|  118.6|   0.33|      0|      11|    7|        2|   0|2015-03-25 14:05:...|    10|      1|[20.0,0.0,38.0,11...|\n",
      "|004D14E5-FAD5-43E...| 26|  1|      279|   53.2|   0.16|      0|       6|    7|        1|   0|2014-12-02 17:22:...|     9|      1|[26.0,1.0,279.0,5...|\n",
      "|00CE4619-8373-490...| 22|  0|      292|   73.7|   0.63|      0|      29|    7|        1|   0|2014-10-01 08:15:...|    11|      1|[22.0,0.0,292.0,7...|\n",
      "|00F65992-27E0-4E3...| 32|  1|      204|  240.5|   0.11|      0|      11|    7|        2|   0|2015-10-28 13:22:...|    25|      0|[32.0,1.0,204.0,2...|\n",
      "|01108838-6A61-427...| 20|  1|       21|   95.6|   0.33|      0|       8|    7|        0|   0|2016-09-20 16:33:...|     6|      1|[20.0,1.0,21.0,95...|\n",
      "|0111E20D-C2D4-4DF...| 50|  0|       15| 1238.4|   1.22|      0|     163|    7|        3|   0|2015-01-19 19:21:...|    34|      0|[50.0,0.0,15.0,12...|\n",
      "|01124D01-66BF-4DE...| 21|  0|      133|   73.6|   0.62|      0|      15|    7|        1|   0|2015-03-04 18:10:...|     6|      1|[21.0,0.0,133.0,7...|\n",
      "|011481B1-E9C3-451...| 43|  1|        1|   76.6|   0.34|      0|       5|    7|        0|   0|2016-01-23 11:05:...|     4|      1|[43.0,1.0,1.0,76....|\n",
      "|01173153-FA2E-44C...| 23|  0|       25|   45.1|   0.04|      0|       1|    7|        0|   0|2016-10-12 14:50:...|     6|      1|[23.0,0.0,25.0,45...|\n",
      "|011BB262-23CA-461...| 41|  0|        7|  614.2|   2.69|      0|     147|    7|        1|   0|2016-09-16 14:00:...|    14|      0|[41.0,0.0,7.0,614...|\n",
      "|0150570B-FA4F-460...| 23|  1|       13|   93.6|    0.6|      0|      13|    7|        0|   0|2016-03-02 20:12:...|     5|      1|[23.0,1.0,13.0,93...|\n",
      "|01514DAA-90B5-4CB...| 29|  1|       31|  101.6|   1.34|      0|      29|    7|        0|   0|2016-03-02 12:49:...|     5|      1|[29.0,1.0,31.0,10...|\n",
      "|0154C884-8608-4D6...| 49|  0|       47|  233.6|   0.79|      0|      24|    4|        1|   0|2015-01-20 17:24:...|     8|      1|[49.0,0.0,47.0,23...|\n",
      "|016BB3C9-A44F-467...| 35|  0|       51|  218.6|   0.68|      0|      20|    4|        1|   0|2015-01-27 10:28:...|     8|      1|[35.0,0.0,51.0,21...|\n",
      "|017435AE-F2EB-4AB...| 20|  0|       13|   82.6|   0.61|      0|      14|    7|        1|   0|2016-04-27 10:09:...|     7|      1|[20.0,0.0,13.0,82...|\n",
      "+--------------------+---+---+---------+-------+-------+-------+--------+-----+---------+----+--------------------+------+-------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "assembler = VectorAssembler(inputCols=features, outputCol=\"features\")\n",
    "df_assembled = assembler.transform(df_customers)\n",
    "df_assembled.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ae66e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---------+-------+-------+-------+--------+-----+---------+----+------+-------+\n",
      "|age|sex|dayswfreq|tbilled|maccess|freeuse|nentries|cfreq|nrenewals|cref|months|dropout|\n",
      "+---+---+---------+-------+-------+-------+--------+-----+---------+----+------+-------+\n",
      "|  0|  0|        0|      0|      0|      0|       0|    0|        0|   0|     0|      0|\n",
      "+---+---+---------+-------+-------+-------+--------+-----+---------+----+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check null values\n",
    "df_assembled.select([F.count(F.when(F.isnan(c) | F.col(c).isNull(), c)).alias(c) for c in features]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8019cc5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.        , -0.05261184, -0.06114127,  0.26556974,  0.07153221,\n",
       "        0.02573219,  0.09373279, -0.25086728,  0.09222838,  0.0539945 ,\n",
       "        0.08846267, -0.23279579, -0.05261184,  1.        , -0.04695518,\n",
       "        0.03849536,  0.10444157,  0.07207284,  0.09380605,  0.10280766,\n",
       "        0.01727373, -0.0063087 ,  0.04409213, -0.00895751, -0.06114127,\n",
       "       -0.04695518,  1.        , -0.13416639, -0.41439499,  0.0100679 ,\n",
       "       -0.18758926, -0.01153372,  0.23681088, -0.01244277,  0.17923565,\n",
       "        0.18040598,  0.26556974,  0.03849536, -0.13416639,  1.        ,\n",
       "        0.1792693 ,  0.12820067,  0.73227319, -0.13314389,  0.54869229,\n",
       "        0.11083614,  0.68522171, -0.35504036,  0.07153221,  0.10444157,\n",
       "       -0.41439499,  0.1792693 ,  1.        , -0.01988687,  0.46971485,\n",
       "        0.07759188, -0.15047663, -0.01284269, -0.11103708, -0.25033027,\n",
       "        0.02573219,  0.07207284,  0.0100679 ,  0.12820067, -0.01988687,\n",
       "        1.        ,  0.11909698,  0.03229684,  0.15129051,  0.04153064,\n",
       "        0.31350941, -0.13425554,  0.09373279,  0.09380605, -0.18758926,\n",
       "        0.73227319,  0.46971485,  0.11909698,  1.        ,  0.01638398,\n",
       "        0.44682313,  0.04815351,  0.5966746 , -0.27660221, -0.25086728,\n",
       "        0.10280766, -0.01153372, -0.13314389,  0.07759188,  0.03229684,\n",
       "        0.01638398,  1.        , -0.0741505 , -0.07605574, -0.04482907,\n",
       "        0.08321567,  0.09222838,  0.01727373,  0.23681088,  0.54869229,\n",
       "       -0.15047663,  0.15129051,  0.44682313, -0.0741505 ,  1.        ,\n",
       "        0.06510163,  0.83237318, -0.21571103,  0.0539945 , -0.0063087 ,\n",
       "       -0.01244277,  0.11083614, -0.01284269,  0.04153064,  0.04815351,\n",
       "       -0.07605574,  0.06510163,  1.        ,  0.0740922 , -0.01828322,\n",
       "        0.08846267,  0.04409213,  0.17923565,  0.68522171, -0.11103708,\n",
       "        0.31350941,  0.5966746 , -0.04482907,  0.83237318,  0.0740922 ,\n",
       "        1.        , -0.26526817, -0.23279579, -0.00895751,  0.18040598,\n",
       "       -0.35504036, -0.25033027, -0.13425554, -0.27660221,  0.08321567,\n",
       "       -0.21571103, -0.01828322, -0.26526817,  1.        ])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "matrix.collect()[0][\"pearson({})\".format(\"features\")].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87dae6c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.        , -0.05261184, -0.06114127,  0.26556974,  0.07153221,\n",
       "        0.02573219,  0.09373279, -0.25086728,  0.09222838,  0.0539945 ,\n",
       "        0.08846267, -0.23279579, -0.05261184,  1.        , -0.04695518,\n",
       "        0.03849536,  0.10444157,  0.07207284,  0.09380605,  0.10280766,\n",
       "        0.01727373, -0.0063087 ,  0.04409213, -0.00895751, -0.06114127,\n",
       "       -0.04695518,  1.        , -0.13416639, -0.41439499,  0.0100679 ,\n",
       "       -0.18758926, -0.01153372,  0.23681088, -0.01244277,  0.17923565,\n",
       "        0.18040598,  0.26556974,  0.03849536, -0.13416639,  1.        ,\n",
       "        0.1792693 ,  0.12820067,  0.73227319, -0.13314389,  0.54869229,\n",
       "        0.11083614,  0.68522171, -0.35504036,  0.07153221,  0.10444157,\n",
       "       -0.41439499,  0.1792693 ,  1.        , -0.01988687,  0.46971485,\n",
       "        0.07759188, -0.15047663, -0.01284269, -0.11103708, -0.25033027,\n",
       "        0.02573219,  0.07207284,  0.0100679 ,  0.12820067, -0.01988687,\n",
       "        1.        ,  0.11909698,  0.03229684,  0.15129051,  0.04153064,\n",
       "        0.31350941, -0.13425554,  0.09373279,  0.09380605, -0.18758926,\n",
       "        0.73227319,  0.46971485,  0.11909698,  1.        ,  0.01638398,\n",
       "        0.44682313,  0.04815351,  0.5966746 , -0.27660221, -0.25086728,\n",
       "        0.10280766, -0.01153372, -0.13314389,  0.07759188,  0.03229684,\n",
       "        0.01638398,  1.        , -0.0741505 , -0.07605574, -0.04482907,\n",
       "        0.08321567,  0.09222838,  0.01727373,  0.23681088,  0.54869229,\n",
       "       -0.15047663,  0.15129051,  0.44682313, -0.0741505 ,  1.        ,\n",
       "        0.06510163,  0.83237318, -0.21571103,  0.0539945 , -0.0063087 ,\n",
       "       -0.01244277,  0.11083614, -0.01284269,  0.04153064,  0.04815351,\n",
       "       -0.07605574,  0.06510163,  1.        ,  0.0740922 , -0.01828322,\n",
       "        0.08846267,  0.04409213,  0.17923565,  0.68522171, -0.11103708,\n",
       "        0.31350941,  0.5966746 , -0.04482907,  0.83237318,  0.0740922 ,\n",
       "        1.        , -0.26526817, -0.23279579, -0.00895751,  0.18040598,\n",
       "       -0.35504036, -0.25033027, -0.13425554, -0.27660221,  0.08321567,\n",
       "       -0.21571103, -0.01828322, -0.26526817,  1.        ])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "matrix = Correlation.corr(df_assembled, \"features\")\n",
    "result = matrix.collect()[0][\"pearson(features)\"].values\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bfb718c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.052612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.061141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.265570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.071532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>0.083216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>-0.215711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>-0.018283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>-0.265268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>144 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0\n",
       "0    1.000000\n",
       "1   -0.052612\n",
       "2   -0.061141\n",
       "3    0.265570\n",
       "4    0.071532\n",
       "..        ...\n",
       "139  0.083216\n",
       "140 -0.215711\n",
       "141 -0.018283\n",
       "142 -0.265268\n",
       "143  1.000000\n",
       "\n",
       "[144 rows x 1 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f6c8078",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 2, got 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)\n",
      "Cell \u001b[1;32mIn[25], line 5\u001b[0m\n",
      "\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Assuming 'corr_matrix' is your calculated correlation matrix\u001b[39;00m\n",
      "\u001b[0;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m8\u001b[39m))  \u001b[38;5;66;03m# Adjust figure size\u001b[39;00m\n",
      "\u001b[1;32m----> 5\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmatrix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcmap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcoolwarm\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m  \n",
      "\u001b[0;32m      6\u001b[0m plt\u001b[38;5;241m.\u001b[39mcolorbar()  \u001b[38;5;66;03m# Add a colorbar for interpretation\u001b[39;00m\n",
      "\u001b[0;32m      7\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow()\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\sobre\\anaconda3\\envs\\bigdata\\Lib\\site-packages\\matplotlib\\pyplot.py:2456\u001b[0m, in \u001b[0;36mmatshow\u001b[1;34m(A, fignum, **kwargs)\u001b[0m\n",
      "\u001b[0;32m   2452\u001b[0m     ax \u001b[38;5;241m=\u001b[39m gca()\n",
      "\u001b[0;32m   2453\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;32m   2454\u001b[0m     \u001b[38;5;66;03m# Extract actual aspect ratio of array and make appropriately sized\u001b[39;00m\n",
      "\u001b[0;32m   2455\u001b[0m     \u001b[38;5;66;03m# figure.\u001b[39;00m\n",
      "\u001b[1;32m-> 2456\u001b[0m     fig \u001b[38;5;241m=\u001b[39m figure(fignum, figsize\u001b[38;5;241m=\u001b[39m\u001b[43mfigaspect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;32m   2457\u001b[0m     ax \u001b[38;5;241m=\u001b[39m fig\u001b[38;5;241m.\u001b[39madd_axes((\u001b[38;5;241m0.15\u001b[39m, \u001b[38;5;241m0.09\u001b[39m, \u001b[38;5;241m0.775\u001b[39m, \u001b[38;5;241m0.775\u001b[39m))\n",
      "\u001b[0;32m   2458\u001b[0m im \u001b[38;5;241m=\u001b[39m ax\u001b[38;5;241m.\u001b[39mmatshow(A, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\sobre\\anaconda3\\envs\\bigdata\\Lib\\site-packages\\matplotlib\\figure.py:3606\u001b[0m, in \u001b[0;36mfigaspect\u001b[1;34m(arg)\u001b[0m\n",
      "\u001b[0;32m   3604\u001b[0m \u001b[38;5;66;03m# Extract the aspect ratio of the array\u001b[39;00m\n",
      "\u001b[0;32m   3605\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m isarray:\n",
      "\u001b[1;32m-> 3606\u001b[0m     nr, nc \u001b[38;5;241m=\u001b[39m arg\u001b[38;5;241m.\u001b[39mshape[:\u001b[38;5;241m2\u001b[39m]\n",
      "\u001b[0;32m   3607\u001b[0m     arr_ratio \u001b[38;5;241m=\u001b[39m nr \u001b[38;5;241m/\u001b[39m nc\n",
      "\u001b[0;32m   3608\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\n",
      "\u001b[1;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 0)"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming 'corr_matrix' is your calculated correlation matrix\n",
    "plt.figure(figsize=(10, 8))  # Adjust figure size\n",
    "plt.matshow(matrix, cmap='coolwarm')  \n",
    "plt.colorbar()  # Add a colorbar for interpretation\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de427176",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2655697400148703"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#df_customers.corr(col1=features, col2=features)\n",
    "df_customers.corr('age', 'tbilled')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trading",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "04564a1046d95370f7cd6169e1f74c2a4d474222cbbb0e33e7b3055c1815a767"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
